{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ tensor(1, dtype=torch.uint8)\n",
      "- tensor(1, dtype=torch.uint8)\n",
      "* tensor(1, dtype=torch.uint8)\n",
      "/ tensor(1, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# element-wise operation\n",
    "# torch.add/sub/mul/div\n",
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(4)\n",
    "print('+', torch.all(torch.eq(a + b, torch.add(a, b))))\n",
    "print('-', torch.all(torch.eq(a - b, torch.sub(a, b))))\n",
    "print('*', torch.all(torch.eq(a * b, torch.mul(a, b))))\n",
    "print('/', torch.all(torch.eq(a / b, torch.div(a, b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.6817, -2.6817, -2.6817],\n",
      "        [ 0.5276,  0.5276,  0.5276],\n",
      "        [ 3.4451,  3.4451,  3.4451]])\n",
      "tensor([[-2.6817, -2.6817, -2.6817],\n",
      "        [ 0.5276,  0.5276,  0.5276],\n",
      "        [ 3.4451,  3.4451,  3.4451]])\n",
      "tensor([[-2.6817, -2.6817, -2.6817],\n",
      "        [ 0.5276,  0.5276,  0.5276],\n",
      "        [ 3.4451,  3.4451,  3.4451]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "# torch.mm只适合2d tensor\n",
    "# torch.matmul和 @ 适用于包括2d在内的所有满足规则的矩阵乘法\n",
    "# 但是三者对于二维Tensor来说是等价的\n",
    "# b_expand = b.unsqueeze(0).expand(3, 4).t() # [4, 3]\n",
    "b_expand = b.unsqueeze(0).expand_as(a).t()\n",
    "print(torch.mm(a, b_expand))\n",
    "print(torch.matmul(a, b_expand))\n",
    "print(a @ b_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error, torch.mm onply support 2d Tensor operation.\n",
      "torch.Size([4, 3, 28, 32])\n",
      "torch.Size([4, 3, 28, 32])\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "a = torch.rand(4, 3, 28, 64)\n",
    "b = torch.rand(4, 3, 64, 32)\n",
    "try:\n",
    "    c = torch.mm(a, b)\n",
    "except Exception:\n",
    "    print('Error, torch.mm onply support 2d Tensor operation.')\n",
    "d = torch.matmul(a, b)\n",
    "print(d.shape) # [4, 3, 28, 32] # 注意此时只在最后的两维进行矩阵乘法，前面的维度保持不变\n",
    "e = torch.rand(3, 64, 32)\n",
    "f = torch.matmul(a, e) # matmul支持自动broadcasting，e先->[1, 3, 64, 32] -> [4, 3, 64, 32]，所以output shape=[4,3, 28, 32]\n",
    "print(f.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]]) tensor([[9., 9.],\n",
      "        [9., 9.]])\n",
      "tensor([[1.7321, 1.7321],\n",
      "        [1.7321, 1.7321]]) tensor([[1.7321, 1.7321],\n",
      "        [1.7321, 1.7321]])\n",
      "tensor([[0.5774, 0.5774],\n",
      "        [0.5774, 0.5774]])\n"
     ]
    }
   ],
   "source": [
    "# power\n",
    "a = torch.full([2, 2], 3)\n",
    "print(a ** 2, a.pow(2)) # 2代表为平方，所以3就是立方。以此类推\n",
    "print(a ** 0.5, a.sqrt())\n",
    "print(torch.rsqrt(a)) # 平方根的倒数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7183, 2.7183, 2.7183],\n",
      "        [2.7183, 2.7183, 2.7183],\n",
      "        [2.7183, 2.7183, 2.7183]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# exp/log\n",
    "a = torch.ones(3, 3)\n",
    "print(torch.exp(a)) # element-wise: e^i for i in a\n",
    "b = torch.log(a) # 默认是以e为底的，2为底:torch.log2，10为底:torch.log10\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(3.) tensor(4.) tensor(3.) tensor(0.1400) tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "# Approximation\n",
    "a = torch.tensor(3.14)\n",
    "print(a.dim())\n",
    "print(a.floor(), a.ceil(), a.trunc(), a.frac(),a.round()) # 下取整，上取整，截断，小数部分，四舍五入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.7749, 11.1175,  5.3519],\n",
      "        [ 8.3907,  8.9612,  4.0661]])\n",
      "min:  tensor(4.0661) max:  tensor(11.7749)\n",
      "tensor([[11.7749, 11.1175, 10.0000],\n",
      "        [10.0000, 10.0000, 10.0000]])\n",
      "tensor([[10.0000, 10.0000,  5.3519],\n",
      "        [ 8.3907,  8.9612,  4.0661]])\n"
     ]
    }
   ],
   "source": [
    "# clamp\n",
    "# 该函数的意义类似np.clip，只不过在传入一个参数的时候默认是看成最小值的限制值\n",
    "grad = torch.rand(2, 3) * 15\n",
    "print(grad)\n",
    "max_ = torch.max(grad)\n",
    "min_ = torch.min(grad)\n",
    "print('min: ', min_, 'max: ', max_)\n",
    "grad_min = grad.clamp(10) # Tensor中小于10的全部拉成10\n",
    "print(grad_min)\n",
    "grad = grad.clamp(0, 10) # Tensor中小于0的全部拉成0，大于10的全部拉成10\n",
    "print(grad)\n",
    "\n",
    "# 小技巧\n",
    "# 在梯度爆炸的时候，一般是权重(w)太大了，导致梯度爆炸，此时我们可以打印  权重的梯度的二阶范数！一般应该在10以内！\n",
    "# 打印方法：print(w.grad.norm(2))  2代表的2阶范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
