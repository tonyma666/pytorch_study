{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 784])\n",
      "torch.Size([112, 28])\n"
     ]
    }
   ],
   "source": [
    "# view or reshape\n",
    "# 每个view都有着他自己的意义的！真的很对！\n",
    "# 另外view里面的 prod(rank) 必须和 prod(origin_rank)相等\n",
    "a = torch.rand(4, 1, 28, 28)\n",
    "print(a.shape)\n",
    "b = a.view(4, 28 * 28)\n",
    "print(b.shape)\n",
    "c = a.view(4 * 28, 28) # 此时我相当于只关心每行的元素\n",
    "print(c.shape)\n",
    "d = a.view(4 * 1, 28, 28) # 此时相当于只关心所有的feature map\n",
    "\n",
    "# 注意这里的 意义 的含义\n",
    "e = d.view(4, 28, 28, 1) # 在操作上没问题的，但是意义错了，造成了数据错乱！！有逻辑错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28, 1])\n",
      "torch.Size([4, 1, 28, 1, 28])\n",
      "torch.Size([1, 4, 1, 28, 28])\n",
      "------------------------------\n",
      "torch.Size([4, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# squeeze/unsqueeze\n",
    "# 注意unsqueeze正索引和负索引的意义是不一样的，正索引表示在 ‘前’插入， 负索引表示在 ‘后’插入\n",
    "# 所以索引的范围：比如rank=3，那么索引的范围就是 [-3, 2] \n",
    "# unsqueeze\n",
    "b = a.unsqueeze(0)\n",
    "print(b.shape) # [1, 4, 1, 28, 28]\n",
    "c = a.unsqueeze(-1)\n",
    "print(c.shape) # [4, 1, 28, 28, 1]  # 之后\n",
    "d = a.unsqueeze(3)\n",
    "print(d.shape) # [4, 1, 28, 1, 28]\n",
    "e = a.unsqueeze(-5)\n",
    "print(e.shape) # [1, 4, 1, 28, 28]\n",
    "print('-' * 30)\n",
    "\n",
    "# squeeze\n",
    "# 注意squeeze在传入非1的维度的时候，什么也不做，并不会报错\n",
    "f = e.squeeze() # 任何索引都不传，能消的全消\n",
    "print(f.shape)\n",
    "g = e.squeeze(0)\n",
    "print(g.shape)\n",
    "h = g.squeeze(1)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4, 4])\n",
      "torch.Size([4, 3, 4, 4])\n",
      "------------------------------\n",
      "torch.Size([1, 9, 4, 4])\n",
      "torch.Size([1, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# expand/repeat\n",
    "a = torch.randn(1, 3, 4, 4)\n",
    "b = torch.ones(3)\n",
    "b_unsqueeze = b.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "# expand\n",
    "# 仅仅是对数据进行复制，使得它们能相加\n",
    "# 注意只能对rank中值为1的维度进行复制，其他的数会报错\n",
    "# 另外如果对当前维度不像expand，写成-1即可\n",
    "b_unsqueeze_expand = b_unsqueeze.expand(1, 3, 4, 4) # [1, 3, 1, 1] -> [1, 3, 4, 4]\n",
    "c = a + b_unsqueeze_expand\n",
    "print(c.shape)\n",
    "c_expand = c.expand(4, 3, -1, -1) # [4, 3, 4, 4]\n",
    "print(c_expand.shape)\n",
    "\n",
    "print('-' * 30)\n",
    "# 一般不建议使用repeat函数进行数据复制，因为它会申请新的内存去存数据，把当前数据复制到别的更大的地方\n",
    "# repeat中的参数的含义表示 该维度的数据需要赋值几次\n",
    "b_unsqueeze_repeat = b_unsqueeze.repeat(1, 3, 4, 4) # [1, 9, 4, 4]，此时的参数和expand相同\n",
    "print(b_unsqueeze_repeat.shape)\n",
    "# 正确的用法\n",
    "b_unsqueeze_repeat = b_unsqueeze.repeat(1, 1, 4, 4)\n",
    "print(b_unsqueeze_repeat.shape) # [1, 3, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7009, -0.6653, -1.0090],\n",
      "        [-0.4819, -2.3559, -0.6941]])\n",
      "tensor([[-0.7009, -0.4819],\n",
      "        [-0.6653, -2.3559],\n",
      "        [-1.0090, -0.6941]])\n"
     ]
    }
   ],
   "source": [
    "# .t\n",
    "# 转置操作只使用于2D Tensor，任何别的维度的Tensor都会报错\n",
    "a = torch.randn(2, 3)\n",
    "print(a)\n",
    "b = a.t()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 32, 3])\n",
      "tensor(1, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# transpose操作\n",
    "# 更加通用的转置操作，不仅仅是2D Tensor.\n",
    "# 注意transpose操作后，Tensor有可能变得不是在一个连续的内存空间中了，调用.view()操作就会报错，因此一般的，在\n",
    "# 调用transpose操作后都会用.contiguous()来让Tensor变得在同一块连续的内存空间中\n",
    "a = torch.randn(4, 3, 32, 32, dtype=torch.float)\n",
    "a1 = a.transpose(1, 3) # 将rank1和rank3进行交换，[4, 32, 32, 3]\n",
    "print(a1.shape)\n",
    "a2 = a1.contiguous() # 变得连续，注意这步操作非常重要，否则会报错\n",
    "a3 = a2.view(4, 3 * 32 * 32)\n",
    "a4 = a3.view(4, 32, 32, 3)\n",
    "a5 = a4.transpose(1, 3)  # 兜了一圈，哈哈，一定要注意那个问题：就是view的逻辑意义！不要丢失信息\n",
    "# validation\n",
    "# torch.eq 判断两个Tensor的值是否都相等，element-wise的操作\n",
    "# torch.all 如果全为True，或者Tensor中全部元素都不为0，则范围tensor(1, dtype=torch.uint8)\n",
    "print(torch.all(torch.eq(a5, a))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "torch.Size([4, 32, 32, 3])\n"
     ]
    }
   ],
   "source": [
    "# permute\n",
    "a = torch.Tensor(4, 3, 32, 32) # [N, C, H, W]\n",
    "print(a.shape)\n",
    "b = a.permute([0, 2, 3, 1]) # [N, H, W, C]\n",
    "print(b.shape)\n",
    "b_reshape = b.contiguous().reshape(4, -1) # 一定要注意permute和transpose方法可能会让Tensor的内存空间变得不连续，要调用contiguous方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
