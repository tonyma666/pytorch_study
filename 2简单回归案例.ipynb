{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本节问题是一个close function问题，即有明确的数学公式，所以用不到实际的loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不用pytorch\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_for_line_given_points(b, w, points):\n",
    "    # x1 -> y1, x2 -> y2, ...\n",
    "    total_error = 0\n",
    "    for i in range(len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        total_error += (w * x + b - y) ** 2 # 计算每个点的MSE，然后累加\n",
    "    return total_error / len(points) # 求得数据集上的平均MSE损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$loss = \\frac{1}{2N}\\sum_{i=1}^{n}{(wx+b-y)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(b_current, w_current, points, learning_rate):\n",
    "    b_gradient = w_gradient = 0\n",
    "    N = len(points)\n",
    "    for i in range(N):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        w_gradient += (w_current * x + b_current - y) * x  # 直接求导法则\n",
    "        b_gradient += (w_current * x + b_current - y) * 1  \n",
    "    # 一定要记住求的是batch_size下的平均梯度\n",
    "    w_gradient /= 2 * N\n",
    "    b_gradient /= 2 * N  # 获得batch_size(其实是全部数据集)上的平均梯度\n",
    "    # 更新梯度\n",
    "    new_w = w_current - learning_rate * w_gradient \n",
    "    new_b = b_current - learning_rate * b_gradient\n",
    "    return [new_b, new_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代多少个step的梯度下降呢？\n",
    "def gradient_decent_runner(points,starting_b, starting_w, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_gradient(b, w, points, learning_rate)\n",
    "    return [b, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before, w=0, b=0, loss=5565.107834483211\n",
      "after, w=1.475996, b=0.177896, loss=112.562030\n"
     ]
    }
   ],
   "source": [
    "# start my journey\n",
    "points = np.genfromtxt('./data/data.csv', delimiter=',') # genfrontxt方法好好哦，要记住\n",
    "learning_rate = 0.0001\n",
    "initial_w = initial_b = 0 \n",
    "print('before, w={}, b={}, loss={}'.format(initial_w, initial_b, compute_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "b, w = gradient_decent_runner(points, initial_b, initial_w, learning_rate, 5000)\n",
    "print('after, w={:2f}, b={:2f}, loss={:4f}'.format(w, b, compute_error_for_line_given_points(b, w, points)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
